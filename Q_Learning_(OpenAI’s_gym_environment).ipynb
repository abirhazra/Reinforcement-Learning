{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q-Learning (OpenAIâ€™s gym environment).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNWu9JasBGGamMkoRTLD8fa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abirhazra/Reinforcement-Learning/blob/main/Q_Learning_(OpenAI%E2%80%99s_gym_environment).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUG1y1NQxUxy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c7b737b-f14b-4e26-db37-b786250712c1"
      },
      "source": [
        "pip install gym"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.19.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2fDG5Kh9XrP"
      },
      "source": [
        "import gym\r\n",
        "import itertools\r\n",
        "import matplotlib\r\n",
        "import matplotlib.style\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import sys\r\n",
        "\r\n",
        "from collections import defaultdict\r\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwEo17SQ-QZq"
      },
      "source": [
        "matplotlib.style.use('ggplot')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfKNeg_J_kvC",
        "outputId": "23619856-8546-4e40-a06f-7592b8ede216"
      },
      "source": [
        "pip install plotting"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: plotting in /usr/local/lib/python3.6/dist-packages (0.0.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from plotting) (1.1.5)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from plotting) (0.11.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from plotting) (3.2.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->plotting) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->plotting) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas->plotting) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.6/dist-packages (from seaborn->plotting) (1.4.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->plotting) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->plotting) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->plotting) (2.4.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->plotting) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGCrCiWf_8GU"
      },
      "source": [
        "import plotting"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHbpNSfpANnt"
      },
      "source": [
        "from collections import defaultdict "
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bohiE0zvAdXk"
      },
      "source": [
        "import gym\r\n",
        "import numpy as np\r\n",
        "import sys\r\n",
        "from gym.envs.toy_text import discrete\r\n",
        "\r\n",
        "UP = 0\r\n",
        "RIGHT = 1\r\n",
        "DOWN = 2\r\n",
        "LEFT = 3\r\n",
        "\r\n",
        "class WindyGridworldEnv(discrete.DiscreteEnv):\r\n",
        "\r\n",
        "    metadata = {'render.modes': ['human', 'ansi']}\r\n",
        "\r\n",
        "    def _limit_coordinates(self, coord):\r\n",
        "        coord[0] = min(coord[0], self.shape[0] - 1)\r\n",
        "        coord[0] = max(coord[0], 0)\r\n",
        "        coord[1] = min(coord[1], self.shape[1] - 1)\r\n",
        "        coord[1] = max(coord[1], 0)\r\n",
        "        return coord\r\n",
        "\r\n",
        "    def _calculate_transition_prob(self, current, delta, winds):\r\n",
        "        new_position = np.array(current) + np.array(delta) + np.array([-1, 0]) * winds[tuple(current)]\r\n",
        "        new_position = self._limit_coordinates(new_position).astype(int)\r\n",
        "        new_state = np.ravel_multi_index(tuple(new_position), self.shape)\r\n",
        "        is_done = tuple(new_position) == (3, 7)\r\n",
        "        return [(1.0, new_state, -1.0, is_done)]\r\n",
        "\r\n",
        "    def __init__(self):\r\n",
        "        self.shape = (7, 10)\r\n",
        "\r\n",
        "        nS = np.prod(self.shape)\r\n",
        "        nA = 4\r\n",
        "\r\n",
        "        # Wind strength\r\n",
        "        winds = np.zeros(self.shape)\r\n",
        "        winds[:,[3,4,5,8]] = 1\r\n",
        "        winds[:,[6,7]] = 2\r\n",
        "\r\n",
        "        # Calculate transition probabilities\r\n",
        "        P = {}\r\n",
        "        for s in range(nS):\r\n",
        "            position = np.unravel_index(s, self.shape)\r\n",
        "            P[s] = { a : [] for a in range(nA) }\r\n",
        "            P[s][UP] = self._calculate_transition_prob(position, [-1, 0], winds)\r\n",
        "            P[s][RIGHT] = self._calculate_transition_prob(position, [0, 1], winds)\r\n",
        "            P[s][DOWN] = self._calculate_transition_prob(position, [1, 0], winds)\r\n",
        "            P[s][LEFT] = self._calculate_transition_prob(position, [0, -1], winds)\r\n",
        "\r\n",
        "        # We always start in state (3, 0)\r\n",
        "        isd = np.zeros(nS)\r\n",
        "        isd[np.ravel_multi_index((3,0), self.shape)] = 1.0\r\n",
        "\r\n",
        "        super(WindyGridworldEnv, self).__init__(nS, nA, P, isd)\r\n",
        "\r\n",
        "    def render(self, mode='human', close=False):\r\n",
        "        self._render(mode, close)\r\n",
        "\r\n",
        "    def _render(self, mode='human', close=False):\r\n",
        "        if close:\r\n",
        "            return\r\n",
        "\r\n",
        "        outfile = StringIO() if mode == 'ansi' else sys.stdout\r\n",
        "\r\n",
        "        for s in range(self.nS):\r\n",
        "            position = np.unravel_index(s, self.shape)\r\n",
        "            # print(self.s)\r\n",
        "            if self.s == s:\r\n",
        "                output = \" x \"\r\n",
        "            elif position == (3,7):\r\n",
        "                output = \" T \"\r\n",
        "            else:\r\n",
        "                output = \" o \"\r\n",
        "\r\n",
        "            if position[1] == 0:\r\n",
        "                output = output.lstrip()\r\n",
        "            if position[1] == self.shape[1] - 1:\r\n",
        "                output = output.rstrip()\r\n",
        "                output += \"\\n\"\r\n",
        "\r\n",
        "            outfile.write(output)\r\n",
        "        outfile.write(\"\\n\")"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2H1JV2u4ARnc"
      },
      "source": [
        "env = WindyGridworldEnv() "
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqCbXjd-AVUj"
      },
      "source": [
        "def createEpsilonGreedyPolicy(Q, epsilon, num_actions): \r\n",
        "    \"\"\" \r\n",
        "    Creates an epsilon-greedy policy based \r\n",
        "    on a given Q-function and epsilon. \r\n",
        "       \r\n",
        "    Returns a function that takes the state \r\n",
        "    as an input and returns the probabilities \r\n",
        "    for each action in the form of a numpy array  \r\n",
        "    of length of the action space(set of possible actions). \r\n",
        "    \"\"\"\r\n",
        "    def policyFunction(state): \r\n",
        "   \r\n",
        "        Action_probabilities = np.ones(num_actions, \r\n",
        "                dtype = float) * epsilon / num_actions \r\n",
        "                  \r\n",
        "        best_action = np.argmax(Q[state]) \r\n",
        "        Action_probabilities[best_action] += (1.0 - epsilon) \r\n",
        "        return Action_probabilities \r\n",
        "   \r\n",
        "    return policyFunction "
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyKj8MEGBAv8"
      },
      "source": [
        "def qLearning(env, num_episodes, discount_factor = 1.0, \r\n",
        "                            alpha = 0.6, epsilon = 0.1): \r\n",
        "    \"\"\" \r\n",
        "    Q-Learning algorithm: Off-policy TD control. \r\n",
        "    Finds the optimal greedy policy while improving \r\n",
        "    following an epsilon-greedy policy\"\"\"\r\n",
        "       \r\n",
        "    # Action value function \r\n",
        "    # A nested dictionary that maps \r\n",
        "    # state -> (action -> action-value). \r\n",
        "    Q = defaultdict(lambda: np.zeros(env.action_space.n)) \r\n",
        "   \r\n",
        "    # Keeps track of useful statistics \r\n",
        "    stats = plotting.EpisodeStats( \r\n",
        "        episode_lengths = np.zeros(num_episodes), \r\n",
        "        episode_rewards = np.zeros(num_episodes))     \r\n",
        "       \r\n",
        "    # Create an epsilon greedy policy function \r\n",
        "    # appropriately for environment action space \r\n",
        "    policy = createEpsilonGreedyPolicy(Q, epsilon, env.action_space.n) \r\n",
        "       \r\n",
        "    # For every episode \r\n",
        "    for ith_episode in range(num_episodes): \r\n",
        "           \r\n",
        "        # Reset the environment and pick the first action \r\n",
        "        state = env.reset() \r\n",
        "           \r\n",
        "        for t in itertools.count(): \r\n",
        "               \r\n",
        "            # get probabilities of all actions from current state \r\n",
        "            action_probabilities = policy(state) \r\n",
        "   \r\n",
        "            # choose action according to  \r\n",
        "            # the probability distribution \r\n",
        "            action = np.random.choice(np.arange( \r\n",
        "                      len(action_probabilities)), \r\n",
        "                       p = action_probabilities) \r\n",
        "   \r\n",
        "            # take action and get reward, transit to next state \r\n",
        "            next_state, reward, done, _ = env.step(action) \r\n",
        "   \r\n",
        "            # Update statistics \r\n",
        "            stats.episode_rewards[ith_episode] += reward \r\n",
        "            stats.episode_lengths[ith_episode] = t \r\n",
        "               \r\n",
        "            # TD Update \r\n",
        "            best_next_action = np.argmax(Q[next_state])     \r\n",
        "            td_target = reward + discount_factor * Q[next_state][best_next_action] \r\n",
        "            td_delta = td_target - Q[state][action] \r\n",
        "            Q[state][action] += alpha * td_delta \r\n",
        "   \r\n",
        "            # done is True if episode terminated    \r\n",
        "            if done: \r\n",
        "                break\r\n",
        "                   \r\n",
        "            state = next_state \r\n",
        "       \r\n",
        "    return Q, stats "
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "r5HvGfZUBEy-",
        "outputId": "20be5431-81f5-4ce2-97d4-74107fc76a37"
      },
      "source": [
        "Q, stats = qLearning(env, 1000) "
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-48d6b0ea09a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqLearning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-33-8b6f56da804d>\u001b[0m in \u001b[0;36mqLearning\u001b[0;34m(env, num_episodes, discount_factor, alpha, epsilon)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Keeps track of useful statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     stats = plotting.EpisodeStats( \n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mepisode_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         episode_rewards = np.zeros(num_episodes))     \n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'plotting' has no attribute 'EpisodeStats'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "quqPQu_VBGbs",
        "outputId": "fe9c7b64-20da-4c9f-8697-554f8fa90012"
      },
      "source": [
        "plotting.plot_episode_stats(stats)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-47362bd72581>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplotting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_episode_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: module 'plotting' has no attribute 'plot_episode_stats'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zzccwmIBvM1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}